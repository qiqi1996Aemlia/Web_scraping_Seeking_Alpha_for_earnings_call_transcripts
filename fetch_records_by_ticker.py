from playwright.sync_api import sync_playwright
import time
import json
import threading
import random


stop_signal = False  # ç”¨äºä¸­æ–­æ»šåŠ¨çº¿ç¨‹

def listen_for_enter():
    global stop_signal
    input("ğŸ“¨ æŒ‰ Enter åœæ­¢è‡ªåŠ¨æ»‘åŠ¨å¹¶å¼€å§‹æå–å†…å®¹...")
    stop_signal = True

def scroll_page(page):
    prev_height = 0
    same_count = 0
    max_same = 5  # è¿ç»­ 5 æ¬¡æ»‘åŠ¨é«˜åº¦æœªå˜åŒ–ï¼Œè®¤å®šåˆ°åº•

    while True:
        # éšæœºæ»šè½®æ»‘åŠ¨ï¼šæ¨¡æ‹Ÿäººè¡Œä¸º
        delta_y = random.randint(500, 3000)
        page.mouse.wheel(0, delta_y)
        time.sleep(random.uniform(1.2, 2.5))

        # æ£€æŸ¥å½“å‰é¡µé¢å¯è§†é«˜åº¦ï¼Œè®¡ç®—æ»šåŠ¨ä½ç½®
        curr_height = page.evaluate("() => window.innerHeight + window.scrollY")

        # æŒç»­åˆ°åº•çš„åˆ¤æ–­
        if curr_height == prev_height:
            same_count += 1
            if same_count >= max_same:
                print("ğŸ›‘ å¯èƒ½å·²åˆ°åº•ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
                break
        else:
            same_count = 0
            prev_height = curr_height
def connect_to_existing_chrome_and_scrape(target_url, ticker_name):
    with sync_playwright() as p:
        # è¿æ¥åˆ°å·²å¼€å¯è¿œç¨‹è°ƒè¯•ç«¯å£çš„ Chrome
        browser = p.chromium.connect_over_cdp("http://localhost:9333")

        # # è·å–æ‰€æœ‰å·²æœ‰é¡µé¢
        context = browser.contexts[0]
        pages = context.pages
        

        # æ‰¾åˆ°å½“å‰æ­£åœ¨æµè§ˆ SeekingAlpha çš„é¡µé¢ï¼ˆå¯æŒ‰æ ‡é¢˜æˆ– URL åŒ¹é…ï¼‰
        for page in pages:
            if "seekingalpha.com" in page.url:
                # print(f"âœ… æ‰¾åˆ°ç›®æ ‡é¡µé¢: {page.url}")

                break
        else:
            raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å« seekingalpha.com çš„é¡µé¢ï¼Œè¯·å…ˆç”¨ Chrome æ‰“å¼€è¯¥é¡µé¢")
        page.goto(target_url, timeout=60000)
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨æ»šåŠ¨é¡µé¢åŠ è½½å†…å®¹...")


        # å¯åŠ¨æ»šåŠ¨çº¿ç¨‹
        scroll_page(page)


        articles = page.query_selector_all('article[data-test-id="post-list-item"]')
        print(f"ğŸ“„ å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        results = []
        for article in articles:
            title_elem = article.query_selector('a[data-test-id="post-list-item-title"]')
            date_elem = article.query_selector('span[data-test-id="post-list-date"]')
            symbol_elems = article.query_selector_all('a[data-test-id="post-list-ticker"]')

            title = title_elem.inner_text().strip() if title_elem else None
            link = title_elem.get_attribute('href') if title_elem else None
            date = date_elem.inner_text().strip() if date_elem else None
            symbols = [s.inner_text().strip() for s in symbol_elems]

            results.append({
                "title": title,
                "link": f"https://seekingalpha.com{link}" if link else None,
                "date": date,
                "symbols": symbols,
            })

        print(json.dumps(results[:3], indent=2))
        with open(f"/Users/qiqi1996amelia/Downloads/seekingalpha_articles_{ticker_name}.json", "w") as f:
            json.dump(results, f, indent=2)

        print("âœ… æ•°æ®ä¿å­˜å®Œæˆ")

if __name__ == "__main__":
    ticker_list = [
       'NTGR', 'NEU', 'NWL', 'NKE', 'NDSN', 'NWPX', 'NCLH', 'NOV', 'NRG',
       'NUS', 'NUE', 'OI', 'OXY', 'OII', 'ODC', 'OIS', 'ODFL', 'OSBC',
       'OLN', 'ZEUS', 'OMC', 'OTEX', 'KAR', 'ORN', 'OSK', 'OSIS', 'OMI'
       ]
    for ticker in ticker_list:
     
        url = f'https://seekingalpha.com/author/sa-transcripts/analysis?ticker={ticker}'
        connect_to_existing_chrome_and_scrape(url, ticker)
